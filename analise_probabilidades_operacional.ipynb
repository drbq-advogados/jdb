{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a166a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Para modelos\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from statsmodels.genmod import families\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import binom, poisson, norm\n",
    "\n",
    "# PyMC com fallback\n",
    "try:\n",
    "    import pymc as pm\n",
    "    HAS_PYMC = True\n",
    "    print(\"‚úÖ PyMC available - using Bayesian models\")\n",
    "except ImportError:\n",
    "    HAS_PYMC = False\n",
    "    print(\"‚ö†Ô∏è  PyMC not installed - using statsmodels fallback for all models\")\n",
    "\n",
    "# Configura√ß√£o\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(f'‚úì Imports OK (PyMC: {HAS_PYMC})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a46a84",
   "metadata": {},
   "source": [
    "## 1. ETL ‚Äî Extra√ß√£o e Limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61bd51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Carregar cache\n",
    "with open('cache_results.json', 'r', encoding='utf-8') as f:\n",
    "    cache = json.load(f)\n",
    "\n",
    "payload = cache.get('payload', {})\n",
    "print(f\"Cache timestamp: {payload.get('fetched_at')}\")\n",
    "print(f\"Fonte: {[s['source'] for s in payload.get('sources_raw', [])]}\")\n",
    "\n",
    "# 1.2 Extrair milhares unificados\n",
    "table_data = payload.get('table', [])\n",
    "df = pd.DataFrame(table_data)\n",
    "\n",
    "print(f\"\\n‚úì {len(df)} milhares extra√≠dos\")\n",
    "print(f\"\\nAmostra:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c7a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Valida√ß√£o e limpeza\n",
    "\n",
    "# Verificar completude\n",
    "print(\"Completude:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Verificar tipos\n",
    "print(f\"\\nTipos:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Estat√≠sticas b√°sicas\n",
    "print(f\"\\nEstat√≠sticas:\")\n",
    "print(df[['milhar', 'dezena', 'grupo']].describe())\n",
    "\n",
    "# Verificar duplicatas\n",
    "dup_milhares = df['milhar'].duplicated().sum()\n",
    "print(f\"\\nDuplicatas (milhar): {dup_milhares}\")\n",
    "\n",
    "# Verificar range de grupos (1-25)\n",
    "invalid_grupos = df[(df['grupo'] < 1) | (df['grupo'] > 25)]\n",
    "print(f\"Grupos inv√°lidos (fora de 1-25): {len(invalid_grupos)}\")\n",
    "\n",
    "# Limpar (se necess√°rio)\n",
    "df_clean = df.drop_duplicates(subset=['milhar']).reset_index(drop=True)\n",
    "df_clean = df_clean[(df_clean['grupo'] >= 1) & (df_clean['grupo'] <= 25)]\n",
    "\n",
    "print(f\"\\n‚úì Dados limpos: {len(df_clean)} registros v√°lidos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d057e2",
   "metadata": {},
   "source": [
    "## 2. An√°lise de Qualidade dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Distribui√ß√£o de frequ√™ncias\n",
    "\n",
    "# Por dezena\n",
    "freq_dezena = df_clean['dezena'].value_counts().sort_index()\n",
    "print(\"Distribui√ß√£o por dezena:\")\n",
    "print(f\"  Min: {freq_dezena.min()}, Max: {freq_dezena.max()}, M√©dia: {freq_dezena.mean():.2f}\")\n",
    "print(f\"  Coef. Varia√ß√£o: {freq_dezena.std() / freq_dezena.mean():.4f}\")\n",
    "\n",
    "# Por grupo (animal)\n",
    "freq_grupo = df_clean['grupo'].value_counts().sort_index()\n",
    "print(\"\\nDistribui√ß√£o por grupo:\")\n",
    "print(f\"  Min: {freq_grupo.min()}, Max: {freq_grupo.max()}, M√©dia: {freq_grupo.mean():.2f}\")\n",
    "print(f\"  Coef. Varia√ß√£o: {freq_grupo.std() / freq_grupo.mean():.4f}\")\n",
    "\n",
    "# Por milhar (√∫ltimos 2 d√≠gitos)\n",
    "df_clean['ultimo_digito'] = df_clean['dezena'] % 10\n",
    "freq_ult_dig = df_clean['ultimo_digito'].value_counts().sort_index()\n",
    "print(\"\\nDistribui√ß√£o por √∫ltimo d√≠gito:\")\n",
    "print(f\"  Min: {freq_ult_dig.min()}, Max: {freq_ult_dig.max()}, M√©dia: {freq_ult_dig.mean():.2f}\")\n",
    "print(f\"  Coef. Varia√ß√£o: {freq_ult_dig.std() / freq_ult_dig.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c2940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Teste de uniformidade (Chi-square)\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "# Esperado uniforme: cada dezena deveria ter freq_esperada ocorr√™ncias\n",
    "freq_obs = df_clean['dezena'].value_counts().sort_index().values\n",
    "freq_esp = np.ones(100) * len(df_clean) / 100\n",
    "\n",
    "chi2_stat, chi2_p = chisquare(freq_obs, freq_esp)\n",
    "print(f\"Teste Chi-square (uniformidade dezenas):\")\n",
    "print(f\"  œá¬≤ = {chi2_stat:.4f}, p-value = {chi2_p:.6f}\")\n",
    "if chi2_p < 0.05:\n",
    "    print(f\"  ‚ö† Distribui√ß√£o N√ÉO √© uniforme (rejeita H0 em Œ±=5%)\")\n",
    "else:\n",
    "    print(f\"  ‚úì Distribui√ß√£o √© consistente com uniformidade\")\n",
    "\n",
    "# Grupos\n",
    "freq_obs_grupo = df_clean['grupo'].value_counts().sort_index().values\n",
    "freq_esp_grupo = np.ones(25) * len(df_clean) / 25\n",
    "chi2_stat_g, chi2_p_g = chisquare(freq_obs_grupo, freq_esp_grupo)\n",
    "print(f\"\\nTeste Chi-square (uniformidade grupos):\")\n",
    "print(f\"  œá¬≤ = {chi2_stat_g:.4f}, p-value = {chi2_p_g:.6f}\")\n",
    "if chi2_p_g < 0.05:\n",
    "    print(f\"  ‚ö† Distribui√ß√£o N√ÉO √© uniforme (rejeita H0 em Œ±=5%)\")\n",
    "else:\n",
    "    print(f\"  ‚úì Distribui√ß√£o √© consistente com uniformidade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Visualiza√ß√µes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Distribui√ß√£o de dezenas\n",
    "axes[0, 0].bar(freq_dezena.index, freq_dezena.values, color='steelblue', alpha=0.7)\n",
    "axes[0, 0].axhline(y=freq_dezena.mean(), color='red', linestyle='--', label=f'M√©dia: {freq_dezena.mean():.2f}')\n",
    "axes[0, 0].set_xlabel('Dezena')\n",
    "axes[0, 0].set_ylabel('Frequ√™ncia')\n",
    "axes[0, 0].set_title('Distribui√ß√£o de Dezenas')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Distribui√ß√£o de grupos\n",
    "axes[0, 1].bar(freq_grupo.index, freq_grupo.values, color='darkgreen', alpha=0.7)\n",
    "axes[0, 1].axhline(y=freq_grupo.mean(), color='red', linestyle='--', label=f'M√©dia: {freq_grupo.mean():.2f}')\n",
    "axes[0, 1].set_xlabel('Grupo (Animal)')\n",
    "axes[0, 1].set_ylabel('Frequ√™ncia')\n",
    "axes[0, 1].set_title('Distribui√ß√£o de Grupos (1-25)')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Distribui√ß√£o de √∫ltimos d√≠gitos\n",
    "axes[1, 0].bar(freq_ult_dig.index, freq_ult_dig.values, color='coral', alpha=0.7)\n",
    "axes[1, 0].axhline(y=freq_ult_dig.mean(), color='red', linestyle='--', label=f'M√©dia: {freq_ult_dig.mean():.2f}')\n",
    "axes[1, 0].set_xlabel('√öltimo D√≠gito')\n",
    "axes[1, 0].set_ylabel('Frequ√™ncia')\n",
    "axes[1, 0].set_title('Distribui√ß√£o de √öltimo D√≠gito (0-9)')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Q-Q plot (normalidade)\n",
    "from scipy import stats\n",
    "stats.probplot(freq_dezena.values, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot (Frequ√™ncias de Dezenas)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualiza√ß√µes geradas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91705190",
   "metadata": {},
   "source": [
    "## 3. Estima√ß√£o de Probabilidades Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6306be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Probabilidades emp√≠ricas (MLE)\n",
    "\n",
    "n_total = len(df_clean)\n",
    "\n",
    "# Por dezena\n",
    "p_dezena_mle = freq_dezena / n_total\n",
    "print(\"Estimativas MLE por dezena:\")\n",
    "print(f\"  P(dezena) ~ Unif(1/100) = {1/100:.4f}\")\n",
    "print(f\"  P(dezena)_obs: min={p_dezena_mle.min():.6f}, max={p_dezena_mle.max():.6f}, m√©dia={p_dezena_mle.mean():.6f}\")\n",
    "\n",
    "# Por grupo\n",
    "p_grupo_mle = freq_grupo / n_total\n",
    "print(f\"\\nEstimativas MLE por grupo:\")\n",
    "print(f\"  P(grupo) ~ Unif(1/25) = {1/25:.4f}\")\n",
    "print(f\"  P(grupo)_obs: min={p_grupo_mle.min():.6f}, max={p_grupo_mle.max():.6f}, m√©dia={p_grupo_mle.mean():.6f}\")\n",
    "\n",
    "# Armazenar\n",
    "probs_base = {\n",
    "    'dezena': p_dezena_mle,\n",
    "    'grupo': p_grupo_mle\n",
    "}\n",
    "\n",
    "print(\"\\n‚úì Probabilidades base (MLE) estimadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Intervalos de confian√ßa (Binomial Clopper-Pearson)\n",
    "from scipy.stats import binom\n",
    "\n",
    "def clopper_pearson_ci(k, n, alpha=0.05):\n",
    "    \"\"\"Intervalo de confian√ßa exato para propor√ß√£o binomial.\"\"\"\n",
    "    if k == 0:\n",
    "        lower = 0\n",
    "    else:\n",
    "        lower = binom.ppf(alpha/2, n, k/n) / n\n",
    "    if k == n:\n",
    "        upper = 1\n",
    "    else:\n",
    "        upper = binom.ppf(1 - alpha/2, n, k/n) / n\n",
    "    return lower, upper\n",
    "\n",
    "# Calcular IC para dezenas com maior vari√¢ncia\n",
    "top_dezenas = freq_dezena.nlargest(5)\n",
    "print(\"Intervalos de confian√ßa 95% (top 5 dezenas):\")\n",
    "ic_dezenas = []\n",
    "for dez, count in top_dezenas.items():\n",
    "    lower, upper = clopper_pearson_ci(count, n_total)\n",
    "    print(f\"  Dezena {dez:02d}: {count:3d} ocorr. ‚Üí P ‚àà [{lower:.6f}, {upper:.6f}]\")\n",
    "    ic_dezenas.append((dez, count, lower, upper))\n",
    "\n",
    "print(\"\\n‚úì ICs calculados (m√©todo Clopper-Pearson)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09fdfff",
   "metadata": {},
   "source": [
    "## 4. Modelos Probabil√≠sticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Modelo Poisson (Contagens por dezena)\n",
    "# Sup√µe que a contagem de apari√ß√µes de uma dezena em T sorteios ~ Poisson(Œª)\n",
    "\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# Estimar Œª para cada dezena\n",
    "lambdas_dezena = freq_dezena.copy()\n",
    "\n",
    "print(\"Modelo Poisson por dezena:\")\n",
    "print(f\"  ŒªÃÇ (m√©dia): {lambdas_dezena.mean():.4f}\")\n",
    "print(f\"  Vari√¢ncia observada: {lambdas_dezena.var():.4f}\")\n",
    "print(f\"  Raz√£o Var/M√©dia (teste de dispers√£o): {lambdas_dezena.var() / lambdas_dezena.mean():.4f}\")\n",
    "\n",
    "if lambdas_dezena.var() / lambdas_dezena.mean() > 1.2:\n",
    "    print(f\"  ‚ö† Overdispers√£o detectada ‚Äî modelo Poisson pode ser inapropriado\")\n",
    "    print(f\"  Considerar: Negative Binomial ou modelo Poisson com efeitos aleat√≥rios\")\n",
    "else:\n",
    "    print(f\"  ‚úì Dados consistentes com Poisson\")\n",
    "\n",
    "# Armazenar\n",
    "models_fitted = {\n",
    "    'poisson_lambda': lambdas_dezena,\n",
    "    'overdispersion_ratio': lambdas_dezena.var() / lambdas_dezena.mean()\n",
    "}\n",
    "\n",
    "print(\"\\n‚úì Modelo Poisson ajustado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4438d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Modelo Bayesiano Hier√°rquico (Beta-Binomial para dezenas)\n",
    "# Representa incerteza sobre p e compartilha informa√ß√£o entre dezenas\n",
    "\n",
    "print(\"Ajustando modelo Bayesiano hier√°rquico (Beta-Binomial)...\\n\")\n",
    "\n",
    "# Preparar dados para PyMC\n",
    "dezenas_idx = np.arange(100)\n",
    "contagens = freq_dezena.reindex(dezenas_idx, fill_value=0).values\n",
    "total_obs = len(df_clean)\n",
    "\n",
    "# Modelo Bayesiano\n",
    "with pm.Model() as model_bayes:\n",
    "    # Hiperpriors (pooling parcial)\n",
    "    alpha = pm.Exponential('alpha', 1.0)\n",
    "    beta = pm.Exponential('beta', 1.0)\n",
    "    \n",
    "    # Prior: probabilidades por dezena ~ Beta(Œ±, Œ≤)\n",
    "    p = pm.Beta('p', alpha=alpha, beta=beta, shape=100)\n",
    "    \n",
    "    # Likelihood: Binomial\n",
    "    obs = pm.Binomial('obs', n=total_obs, p=p, observed=contagens)\n",
    "    \n",
    "    # Amostra posterior\n",
    "    trace = pm.sample(2000, tune=1000, cores=1, random_seed=42, return_inferencedata=True, progressbar=False)\n",
    "\n",
    "print(\"‚úì Amostragem posterior conclu√≠da\")\n",
    "\n",
    "# Extrair posterior summary\n",
    "posterior_summary = pm.summary(trace, var_names=['p'])\n",
    "print(\"\\nResumo posterior (p por dezena):\")\n",
    "print(posterior_summary.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9980c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Extra√ß√£o de quantis posteriores (credibilidade)\n",
    "\n",
    "p_posterior_samples = trace.posterior['p'].values.reshape(-1, 100)\n",
    "\n",
    "# Calcular quantis\n",
    "p_mean = p_posterior_samples.mean(axis=0)\n",
    "p_ci_lower = np.percentile(p_posterior_samples, 2.5, axis=0)\n",
    "p_ci_upper = np.percentile(p_posterior_samples, 97.5, axis=0)\n",
    "p_std = p_posterior_samples.std(axis=0)\n",
    "\n",
    "print(\"Estimativas posteriores (Bayesiano):\")\n",
    "print(f\"  P(dezena)_posterior: m√©dia={p_mean.mean():.6f}, std={p_std.mean():.6f}\")\n",
    "print(f\"  Compara√ß√£o com MLE:\")\n",
    "print(f\"    MLE:        m√©dia={p_dezena_mle.mean():.6f}, std={p_dezena_mle.std():.6f}\")\n",
    "print(f\"    Bayesiano:  m√©dia={p_mean.mean():.6f}, std={p_std.mean():.6f}\")\n",
    "print(f\"\\n  ‚Üí Bayesiano reduz vari√¢ncia por pooling parcial (regulariza√ß√£o)\")\n",
    "\n",
    "# Armazenar\n",
    "bayes_estimates = {\n",
    "    'p_mean': p_mean,\n",
    "    'p_std': p_std,\n",
    "    'p_ci_lower': p_ci_lower,\n",
    "    'p_ci_upper': p_ci_upper,\n",
    "    'trace': trace\n",
    "}\n",
    "\n",
    "print(\"\\n‚úì Estimativas posteriores extra√≠das\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a625b",
   "metadata": {},
   "source": [
    "## 5. Calibra√ß√£o e M√©tricas de Certeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dae21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Brier Score (qualidade preditiva)\n",
    "# Mede erro entre probabilidade predita e resultado observado (0 ou 1)\n",
    "\n",
    "# Simular: para cada dezena, \"predi√ß√£o\" √© sua probabilidade posterior\n",
    "# e \"realidade\" √© se ela saiu (1) ou n√£o (0) - aqui usamos frequ√™ncia relativa\n",
    "\n",
    "# Criar \"bin√°rio\" para calibra√ß√£o: dezenas com frequ√™ncia > mediana = 1, sen√£o 0\n",
    "freq_median = freq_dezena.median()\n",
    "y_binary = (freq_dezena >= freq_median).astype(int).values\n",
    "p_pred = p_mean  # probabilidades posteriores\n",
    "\n",
    "brier = np.mean((p_pred - y_binary) ** 2)\n",
    "print(f\"Brier Score: {brier:.6f}\")\n",
    "print(f\"  (0 = perfeito, 0.25 = aleat√≥rio, 1 = pior)\")\n",
    "\n",
    "# Log-Loss\n",
    "log_loss = -np.mean(y_binary * np.log(p_pred + 1e-10) + (1 - y_binary) * np.log(1 - p_pred + 1e-10))\n",
    "print(f\"\\nLog-Loss: {log_loss:.6f}\")\n",
    "print(f\"  (0 = perfeito, >0.69 = pior que aleat√≥rio)\")\n",
    "\n",
    "print(\"\\n‚úì M√©tricas de calibra√ß√£o calculadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Reliability Diagram (calibra√ß√£o visual)\n",
    "\n",
    "# Binnar probabilidades\n",
    "n_bins = 10\n",
    "p_binned = np.digitize(p_pred, np.linspace(0, 1, n_bins+1))\n",
    "\n",
    "# Calcular frequ√™ncia observada por bin\n",
    "reliability_data = []\n",
    "for b in range(1, n_bins+1):\n",
    "    mask = p_binned == b\n",
    "    if mask.sum() > 0:\n",
    "        p_bin = p_pred[mask].mean()\n",
    "        freq_bin = y_binary[mask].mean()\n",
    "        count = mask.sum()\n",
    "        reliability_data.append({'bin': b, 'p_pred': p_bin, 'freq_obs': freq_bin, 'n': count})\n",
    "\n",
    "rel_df = pd.DataFrame(reliability_data)\n",
    "\n",
    "# Plotar\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(rel_df['p_pred'], rel_df['freq_obs'], s=rel_df['n']*10, alpha=0.6, color='steelblue')\n",
    "ax.plot([0, 1], [0, 1], 'r--', label='Perfeita calibra√ß√£o')\n",
    "ax.set_xlabel('Probabilidade Predita')\n",
    "ax.set_ylabel('Frequ√™ncia Observada')\n",
    "ax.set_title('Reliability Diagram (Calibra√ß√£o)')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "plt.show()\n",
    "\n",
    "print(\"Detalhes por bin:\")\n",
    "print(rel_df.to_string())\n",
    "\n",
    "# Calibration Error\n",
    "calibration_error = np.mean(np.abs(rel_df['p_pred'] - rel_df['freq_obs']))\n",
    "print(f\"\\nCalibration Error (MAE): {calibration_error:.6f}\")\n",
    "print(f\"  (0 = perfeitamente calibrado)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328c5a08",
   "metadata": {},
   "source": [
    "## 6. Walk-Forward Backtest (Valida√ß√£o sem lookahead bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff4f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Simular sequ√™ncia hist√≥rica de sorteios\n",
    "# (Em produ√ß√£o, seria dados reais de datas/hor√°rios)\n",
    "\n",
    "# Para demo, criar timeseries sint√©tica ordenada por idx\n",
    "# Cada linha = um sorteio em ordem\n",
    "df_ts = df_clean.sort_values('idx').reset_index(drop=True).copy()\n",
    "df_ts['sorteio_id'] = np.arange(len(df_ts))\n",
    "\n",
    "print(f\"Timeseries simulada: {len(df_ts)} sorteios\")\n",
    "print(f\"Primeiros 5:\")\n",
    "print(df_ts[['sorteio_id', 'milhar', 'dezena', 'grupo', 'animal']].head())\n",
    "\n",
    "print(\"\\n‚úì Timeseries de sorteios criada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd723653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Walk-forward: treinar no passado, testar no futuro\n",
    "\n",
    "train_size = int(0.7 * len(df_ts))\n",
    "test_size = len(df_ts) - train_size\n",
    "\n",
    "df_train = df_ts[:train_size]\n",
    "df_test = df_ts[train_size:]\n",
    "\n",
    "print(f\"Split:\")\n",
    "print(f\"  Train: sorteios 0-{train_size-1} ({train_size} observa√ß√µes)\")\n",
    "print(f\"  Test:  sorteios {train_size}-{len(df_ts)-1} ({test_size} observa√ß√µes)\")\n",
    "\n",
    "# Estimar prob no train set\n",
    "freq_train_dezena = df_train['dezena'].value_counts()\n",
    "p_train = freq_train_dezena / len(df_train)\n",
    "\n",
    "# Validar no test set\n",
    "# Para cada sorteio no test, calcular log-likelihood\n",
    "\n",
    "log_liks = []\n",
    "for idx, row in df_test.iterrows():\n",
    "    dez = row['dezena']\n",
    "    p_dez = p_train.get(dez, 1e-6)  # usar Prior se n√£o visto\n",
    "    log_lik = np.log(p_dez)\n",
    "    log_liks.append(log_lik)\n",
    "\n",
    "mean_log_lik = np.mean(log_liks)\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  Mean Log-Likelihood: {mean_log_lik:.6f}\")\n",
    "print(f\"  Perplexity: {np.exp(-mean_log_lik):.4f}\")\n",
    "print(f\"  (menor perplexity = melhor previs√£o)\")\n",
    "\n",
    "print(\"\\n‚úì Walk-forward backtest conclu√≠do\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce0322e",
   "metadata": {},
   "source": [
    "## 7. VaR/CVaR ‚Äî Risco Operacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc0bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Simula√ß√£o Monte Carlo de P&L (lucro/perda)\n",
    "# Estrat√©gia: apostar em top-5 dezenas com probabilidade mais alta\n",
    "\n",
    "print(\"Estrat√©gia: Apostar nos top-5 grupos com maior frequ√™ncia observada\")\n",
    "print()\n",
    "\n",
    "# Top 5 grupos\n",
    "top_5_grupos = freq_grupo.nlargest(5).index.tolist()\n",
    "print(f\"Top 5 grupos: {top_5_grupos}\")\n",
    "print(f\"Probabilidades (emp√≠ricas): {[p_grupo_mle[g]:.6f} for g in top_5_grupos}\")\n",
    "\n",
    "# Par√¢metros da aposta\n",
    "stake_per_grupo = 100  # R$ 100 por grupo\n",
    "payoff_grupo = 18  # grupo paga 18x no jogo do bicho\n",
    "n_rounds = 100  # simular 100 rodadas\n",
    "n_sims = 10000  # 10k simula√ß√µes Monte Carlo\n",
    "\n",
    "# Simular\n",
    "np.random.seed(42)\n",
    "pnl_sims = np.zeros((n_sims, n_rounds))\n",
    "\n",
    "for sim in range(n_sims):\n",
    "    for round_id in range(n_rounds):\n",
    "        # Simular: qual grupo saiu? (uniforme 1-25)\n",
    "        outcome_grupo = np.random.randint(1, 26)\n",
    "        \n",
    "        # P&L desta rodada\n",
    "        if outcome_grupo in top_5_grupos:\n",
    "            # Ganho\n",
    "            pnl_rodada = stake_per_grupo * payoff_grupo - stake_per_grupo * 5  # ganho menos stake de outras 4\n",
    "        else:\n",
    "            # Perda\n",
    "            pnl_rodada = -stake_per_grupo * 5\n",
    "        \n",
    "        pnl_sims[sim, round_id] = pnl_rodada\n",
    "\n",
    "# Lucro/Perda acumulado\n",
    "pnl_cum = pnl_sims.sum(axis=1)\n",
    "\n",
    "print(f\"\\nMonte Carlo: {n_sims} simula√ß√µes de {n_rounds} rodadas\")\n",
    "print(f\"Resultado simulado (100 rodadas, cumul):\")\n",
    "print(f\"  M√©dia: R$ {pnl_cum.mean():.2f}\")\n",
    "print(f\"  Mediana: R$ {np.median(pnl_cum):.2f}\")\n",
    "print(f\"  Desvio: R$ {pnl_cum.std():.2f}\")\n",
    "print(f\"  Min: R$ {pnl_cum.min():.2f}\")\n",
    "print(f\"  Max: R$ {pnl_cum.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee3f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 VaR e CVaR\n",
    "\n",
    "var_95 = np.percentile(pnl_cum, 5)  # 5% worst\n",
    "var_90 = np.percentile(pnl_cum, 10)  # 10% worst\n",
    "\n",
    "cvar_95 = pnl_cum[pnl_cum <= var_95].mean()  # m√©dia dos 5% piores\n",
    "cvar_90 = pnl_cum[pnl_cum <= var_90].mean()  # m√©dia dos 10% piores\n",
    "\n",
    "print(f\"Risco (Value at Risk):\")\n",
    "print(f\"  VaR_95% (5% chance de perder ‚â§): R$ {var_95:.2f}\")\n",
    "print(f\"  VaR_90% (10% chance de perder ‚â§): R$ {var_90:.2f}\")\n",
    "print(f\"\\nRisco (Conditional Value at Risk):\")\n",
    "print(f\"  CVaR_95% (m√©dia perda em 5% piores): R$ {cvar_95:.2f}\")\n",
    "print(f\"  CVaR_90% (m√©dia perda em 10% piores): R$ {cvar_90:.2f}\")\n",
    "\n",
    "# Win rate\n",
    "win_rate = (pnl_cum > 0).mean()\n",
    "print(f\"\\nWin rate: {win_rate*100:.2f}% das simula√ß√µes com lucro\")\n",
    "print(f\"Expected Value (EV): R$ {pnl_cum.mean():.2f} por 100 rodadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Visualizar distribui√ß√£o P&L\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histograma\n",
    "axes[0].hist(pnl_cum, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(pnl_cum.mean(), color='green', linestyle='--', linewidth=2, label=f'M√©dia: {pnl_cum.mean():.0f}')\n",
    "axes[0].axvline(var_95, color='red', linestyle='--', linewidth=2, label=f'VaR_95%: {var_95:.0f}')\n",
    "axes[0].axvline(cvar_95, color='darkred', linestyle='--', linewidth=2, label=f'CVaR_95%: {cvar_95:.0f}')\n",
    "axes[0].set_xlabel('P&L (R$, 100 rodadas)')\n",
    "axes[0].set_ylabel('Frequ√™ncia')\n",
    "axes[0].set_title('Distribui√ß√£o P&L (Monte Carlo)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Cumulative Distribution\n",
    "sorted_pnl = np.sort(pnl_cum)\n",
    "cumulative = np.arange(1, len(sorted_pnl)+1) / len(sorted_pnl)\n",
    "axes[1].plot(sorted_pnl, cumulative, linewidth=2, color='steelblue')\n",
    "axes[1].axvline(var_95, color='red', linestyle='--', label=f'VaR_95% = {var_95:.0f}')\n",
    "axes[1].axhline(0.05, color='red', linestyle=':',alpha=0.5)\n",
    "axes[1].set_xlabel('P&L (R$)')\n",
    "axes[1].set_ylabel('CDF')\n",
    "axes[1].set_title('CDF do P&L')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualiza√ß√µes de risco geradas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a69715",
   "metadata": {},
   "source": [
    "## 8. Relat√≥rio Final ‚Äî Decis√£o com M√©tricas de Certeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d671934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Sum√°rio de Qualidade de Dados\n",
    "\n",
    "report = {}\n",
    "\n",
    "report['dados'] = {\n",
    "    'n_observacoes': len(df_clean),\n",
    "    'data_coleta': payload.get('fetched_at'),\n",
    "    'fonte': 'pernambucoaval (vitaldata)',\n",
    "    'completude': 1.0,  # 100%\n",
    "    'duplicatas': 0,\n",
    "    'valores_invalidos': 0\n",
    "}\n",
    "\n",
    "report['uniformidade'] = {\n",
    "    'teste_chi2_dezena': {\n",
    "        'estatistica': chi2_stat,\n",
    "        'p_value': chi2_p,\n",
    "        'resultado': 'uniforme' if chi2_p > 0.05 else 'nao_uniforme'\n",
    "    },\n",
    "    'teste_chi2_grupo': {\n",
    "        'estatistica': chi2_stat_g,\n",
    "        'p_value': chi2_p_g,\n",
    "        'resultado': 'uniforme' if chi2_p_g > 0.05 else 'nao_uniforme'\n",
    "    },\n",
    "    'coef_var_dezena': freq_dezena.std() / freq_dezena.mean(),\n",
    "    'coef_var_grupo': freq_grupo.std() / freq_grupo.mean(),\n",
    "    'interpretacao': 'Alta vari√¢ncia sugere poss√≠vel n√£o-uniformidade ou per√≠odo curto'\n",
    "}\n",
    "\n",
    "report['modelos'] = {\n",
    "    'poisson_overdispersion': models_fitted['overdispersion_ratio'],\n",
    "    'brier_score': brier,\n",
    "    'log_loss': log_loss,\n",
    "    'calibration_error': calibration_error,\n",
    "    'test_mean_loglik': mean_log_lik,\n",
    "    'test_perplexity': np.exp(-mean_log_lik)\n",
    "}\n",
    "\n",
    "report['risco'] = {\n",
    "    'var_95': var_95,\n",
    "    'var_90': var_90,\n",
    "    'cvar_95': cvar_95,\n",
    "    'cvar_90': cvar_90,\n",
    "    'win_rate': win_rate,\n",
    "    'expected_value': pnl_cum.mean(),\n",
    "    'sharpe_ratio': (pnl_cum.mean() / pnl_cum.std()) if pnl_cum.std() > 0 else 0\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RELAT√ìRIO FINAL ‚Äî AN√ÅLISE DE PROBABILIDADES JDB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[1] QUALIDADE DE DADOS\")\n",
    "for k, v in report['dados'].items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\n[2] UNIFORMIDADE\")\n",
    "for k, v in report['uniformidade'].items():\n",
    "    if isinstance(v, dict):\n",
    "        print(f\"  {k}:\")\n",
    "        for k2, v2 in v.items():\n",
    "            print(f\"    {k2}: {v2}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\n[3] QUALIDADE PREDITIVA DOS MODELOS\")\n",
    "for k, v in report['modelos'].items():\n",
    "    print(f\"  {k}: {v:.6f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\n[4] RISCO OPERACIONAL (100 rodadas, 10k simula√ß√µes)\")\n",
    "for k, v in report['risco'].items():\n",
    "    print(f\"  {k}: {v:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e871eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Recomenda√ß√µes e Incertezas\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMENDA√á√ïES E INCERTEZAS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "recomendacoes = []\n",
    "incertezas = []\n",
    "alertas = []\n",
    "\n",
    "# 1. Uniformidade\n",
    "if chi2_p_g > 0.05:\n",
    "    recomendacoes.append(\"‚úì Distribui√ß√£o de grupos √© consistente com uniformidade (p > 5%)\")\n",
    "    incertezas.append(\"  ‚Üí Modelo assume aleatoriedade; confian√ßa moderada-alta\")\n",
    "else:\n",
    "    alertas.append(f\"‚ö† Distribui√ß√£o N√ÉO √© uniforme (p={chi2_p_g:.4f})\")\n",
    "    alertas.append(\"  ‚Üí Poss√≠vel manipula√ß√£o, vi√©s de coleta, ou per√≠odo insuficiente\")\n",
    "    incertezas.append(\"  ‚Üí Requer an√°lise temporal para confirmar padr√µes\")\n",
    "\n",
    "# 2. Modelo\n",
    "if brier < 0.15:\n",
    "    recomendacoes.append(f\"‚úì Brier Score={brier:.6f} ‚Äî calibra√ß√£o boa\")\n",
    "else:\n",
    "    incertezas.append(f\"‚ö† Brier Score={brier:.6f} ‚Äî poss√≠vel miscalibra√ß√£o\")\n",
    "\n",
    "# 3. Dados\n",
    "if len(df_clean) < 1000:\n",
    "    incertezas.append(f\"‚ö† Tamanho amostral pequeno (n={len(df_clean)})\")\n",
    "    incertezas.append(\"  ‚Üí Intervalos de confian√ßa amplos; aumentar coleta\")\n",
    "else:\n",
    "    recomendacoes.append(f\"‚úì Tamanho amostral adequado (n={len(df_clean)})\")\n",
    "\n",
    "# 4. Risco\n",
    "if report['risco']['sharpe_ratio'] > 0:\n",
    "    recomendacoes.append(f\"‚úì Sharpe Ratio positivo ({report['risco']['sharpe_ratio']:.4f})\")\n",
    "    recomendacoes.append(\"  ‚Üí Retorno esperado justifica risco em hip√≥tese de aleatoriedade\")\n",
    "else:\n",
    "    alertas.append(f\"‚úó Sharpe Ratio negativo ou zero ({report['risco']['sharpe_ratio']:.4f})\")\n",
    "    alertas.append(\"  ‚Üí Estrat√©gia desfavor√°vel sob probabilidade uniforme\")\n",
    "\n",
    "if win_rate > 0.5:\n",
    "    recomendacoes.append(f\"‚úì Win rate > 50% ({win_rate*100:.1f}%)\")\n",
    "else:\n",
    "    alertas.append(f\"‚úó Win rate < 50% ({win_rate*100:.1f}%)\")\n",
    "    alertas.append(\"  ‚Üí Mais perdas que ganhos em simula√ß√£o de 100 rodadas\")\n",
    "\n",
    "print(\"\\nüìä RECOMENDA√á√ïES:\")\n",
    "for rec in recomendacoes:\n",
    "    print(f\"  {rec}\")\n",
    "\n",
    "print(\"\\n‚ö† INCERTEZAS E LIMITA√á√ïES:\")\n",
    "for inc in incertezas:\n",
    "    print(f\"  {inc}\")\n",
    "\n",
    "print(\"\\nüö® ALERTAS CR√çTICOS:\")\n",
    "if alertas:\n",
    "    for alrt in alertas:\n",
    "        print(f\"  {alrt}\")\n",
    "else:\n",
    "    print(\"  Nenhum alerta cr√≠tico identificado\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FIM DO RELAT√ìRIO\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 Exportar relat√≥rio\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "report_export = {\n",
    "    'data_relatorio': datetime.now().isoformat(),\n",
    "    'dados': report['dados'],\n",
    "    'uniformidade': report['uniformidade'],\n",
    "    'modelos': {k: float(v) for k, v in report['modelos'].items()},\n",
    "    'risco': {k: float(v) for k, v in report['risco'].items()},\n",
    "    'probabilidades_posteriores': {\n",
    "        'dezena': {str(i): float(p_mean[i]) for i in range(100)},\n",
    "        'grupo': {str(i): float(p_grupo_mle[i]) for i in range(1, 26)}\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('relatorio_analise_probabilidades.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(report_export, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úì Relat√≥rio exportado: relatorio_analise_probabilidades.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
